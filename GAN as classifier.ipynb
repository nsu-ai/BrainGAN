{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd msg_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "from MSG_GAN.GAN import *\n",
    "from MSG_GAN.CustomLayers import *\n",
    "from MSG_GAN.utils import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import avg_pool2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 6\n",
    "\n",
    "latent_size = 512\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "n_classes = 4\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "lr = 3e-4\n",
    "\n",
    "gpu_parallelize = True\n",
    "num_workers = 2\n",
    "\n",
    "image_size = 128\n",
    "\n",
    "images_dir = 'msg_gan/data'\n",
    "model_dir = '/output files'\n",
    "samples_dir = '/output files'\n",
    "#ACMSGGAN Discriminator checkpoint file\n",
    "model_path = '/models/GAN_DIS_1450.pth'\n",
    "\n",
    "#model_path = '/content/drive/MyDrive/Colab Notebooks/Brain Tumor Classification/GAN_DIS_650.pth'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(root=images_dir,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.Grayscale(1),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,), (0.5,)),\n",
    "                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = (len(dataset) - train_size) \n",
    "print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size], generator=torch.manual_seed(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b, (images, labels) in enumerate(train_loader):\n",
    "    break\n",
    "\n",
    "class_names = ['Glioma','Menignioma', 'No Tumor', 'Pituitary']\n",
    "print('Label:', labels.numpy())\n",
    "print('Class: ', *np.array([class_names[i] for i in labels]))\n",
    "\n",
    "# Print the images\n",
    "im = vutils.make_grid(images, nrow=8)  # the default nrow is 8\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomDiscriminator(depth, latent_size, n_classes=n_classes).to(device)\n",
    "model = nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Discriminator Network Parameters:\\n\")\n",
    "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Total Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.layers[0].aux = nn.Sequential(\n",
    "    nn.Linear(512, 512),\n",
    "    nn.LeakyReLU(0.2, True),\n",
    "    model.module.layers[0].aux\n",
    ")\n",
    "\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "for params in model.module.layers[0].aux.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Discriminator Network Parameters (After Freezing for fine-tuning as classifier):\\n\")\n",
    "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Total Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main 98.29\n",
    "# Print\n",
    "model_name = 'New_AC_MSGGAN_classifier_Train_Test_Split_dis500_lr_5e-6'\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "Q = math.floor(len(train_loader)/batch_size)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=lr, max_lr=0.1, step_size_up=5, mode=\"exp_range\", gamma=0.85, cycle_momentum=False)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Q)\n",
    "print('\\nTraining the model')\n",
    "b, test_b = 0, 0\n",
    "\n",
    "training_losses = []\n",
    "training_accuracies = []\n",
    "test_loss = []\n",
    "test_corr = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    e_start = time.time()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    tst_corr = 0.0\n",
    "\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        X_train = [X_train] + [avg_pool2d(X_train, int(np.power(2, i)))\n",
    "                                  for i in range(1, depth)]\n",
    "        X_train = list(reversed(X_train))\n",
    "\n",
    "        _, y_pred = model(X_train)\n",
    "\n",
    "        y_pred = y_pred.view(-1, n_classes)\n",
    "\n",
    "        loss = criterion(y_pred, y_train)\n",
    "\n",
    "        predicted = torch.argmax(y_pred.data, dim=1).data\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        running_accuracy += batch_corr\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # scheduler.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if b % int(len(train_loader)/batch_size) == 0:\n",
    "            print(f'Epoch: {epoch+1:2}  batch: {b+1:6} [{b+1:6}/{len(train_loader)}]  Loss: {loss.item():.6f}  Accuracy: {running_accuracy.item()*100/((batch_size ) * (b+1)):.6f}%')\n",
    "        \n",
    "    training_losses.append(loss.item())\n",
    "    training_accuracies.append(running_accuracy.item()*100/((batch_size ) * (b+1)))\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Training Accuracy: {training_accuracies[-1]:.6f}% | Training Loss: {training_losses[-1]:.6f}\")\n",
    "\n",
    "    model.eval()\n",
    "    b = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        labels = []\n",
    "        pred = []\n",
    "\n",
    "        new_y = 0.0\n",
    "\n",
    "        # perform test set evaluation batch wise\n",
    "        for b, (X, y) in enumerate(test_loader):\n",
    "            b += 1\n",
    "            # set label to use CUDA if available\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            X = [X] + [avg_pool2d(X, int(np.power(2, i)))\n",
    "                                  for i in range(1, depth)]\n",
    "            X = list(reversed(X))\n",
    "\n",
    "            labels.extend(y.view(-1).cpu().numpy())\n",
    "\n",
    "            # perform forward pass\n",
    "            _, y_val = model(X)\n",
    "            y_val = y_val.view(-1, n_classes)\n",
    "\n",
    "            # get argmax of predicted values, which is our label\n",
    "            predicted = torch.argmax(y_val.data, dim=1).view(-1)\n",
    "\n",
    "            # append predicted label\n",
    "            pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # calculate loss\n",
    "            loss = criterion(y_val, y)\n",
    "\n",
    "            # increment correct with correcly predicted labels per batch\n",
    "            correct += (predicted == y).sum()\n",
    "\n",
    "        # append correct samples labels and losses\n",
    "        test_corr.append(correct.item()*100/(b*(batch_size )))\n",
    "        \n",
    "        test_loss.append(loss.item())\n",
    "            \n",
    "    print(f\"Validation accuracy: {test_corr[-1]:.6f}% | Validation Loss: {test_loss[-1]:.6f}\")\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "end_time = time.time() - start_time    \n",
    "\n",
    "# print training summary\n",
    "print(\"\\nTraining Duration {:.2f} minutes\".format(end_time/60))\n",
    "print(\"GPU memory used : {} kb\".format(torch.cuda.memory_allocated()))\n",
    "print(\"GPU memory cached : {} kb\".format(torch.cuda.memory_reserved()))\n",
    "\n",
    "plot_loss(training_losses, test_loss) \n",
    "plot_accuracy(training_accuracies, test_corr) \n",
    "\n",
    "#torch.save(model.state_dict(), '/content/drive/My Drive/' + model_name + '.pt')\n",
    "#torch.save(model.state_dict(), model_dir + '/' + model_name + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir = 'msg_gan/test_data'\n",
    "test_dataset = ImageFolder(root=test_images_dir,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.Grayscale(1),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,), (0.5,)),\n",
    "                           ]))\n",
    "print(len(test_dataset))\n",
    "new_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "#Dis -100 & Moved one image number 719 from glioma to no tumor to check the results\n",
    "model.eval()\n",
    "\n",
    "b = 0\n",
    "test_loss = []\n",
    "test_corr = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    labels = []\n",
    "    pred = []\n",
    "\n",
    "    new_y = 0.0\n",
    "\n",
    "    # perform test set evaluation batch wise\n",
    "    for b, (X, y) in enumerate(new_test_loader):\n",
    "        b += 1\n",
    "        # set label to use CUDA if available\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        X = [X] + [avg_pool2d(X, int(np.power(2, i)))\n",
    "                              for i in range(1, depth)]\n",
    "        X = list(reversed(X))\n",
    "\n",
    "        labels.extend(y.view(-1).cpu().numpy())\n",
    "\n",
    "        # perform forward pass\n",
    "        _, y_val = model(X)\n",
    "        y_val = y_val.view(-1, n_classes)\n",
    "\n",
    "        # get argmax of predicted values, which is our label\n",
    "        predicted = torch.argmax(y_val.data, dim=1).view(-1)\n",
    "\n",
    "        # append predicted label\n",
    "        pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # calculate loss\n",
    "        loss = criterion(y_val, y)\n",
    "\n",
    "        # increment correct with correcly predicted labels per batch\n",
    "        correct += (predicted == y).sum()\n",
    "\n",
    "    # append correct samples labels and losses\n",
    "    test_corr.append(correct.item()*100/(b*(batch_size )))\n",
    "    \n",
    "    test_loss.append(loss.item())\n",
    "\n",
    "print(f\"Test Loss: {test_loss[-1]}\")\n",
    "\n",
    "labels = torch.Tensor(labels)\n",
    "pred = torch.Tensor(pred)\n",
    "\n",
    "print(\"Test Metrics: \\n\")\n",
    "\n",
    "get_all_metrics(pred, labels)\n",
    "\n",
    "if class_names:\n",
    "    plot_confusion_matrix(pred, labels, class_names)\n",
    "else:\n",
    "    plot_confusion_matrix(pred, labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
